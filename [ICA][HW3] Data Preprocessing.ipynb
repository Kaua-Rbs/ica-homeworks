{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# PARTE 0 | Importes Necessários"
      ],
      "metadata": {
        "id": "V_LLfWXdaM9z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import kagglehub\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, PowerTransformer"
      ],
      "metadata": {
        "id": "3nwsAdtkaVZ0"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PARTE 1 | Conjunto de Dados"
      ],
      "metadata": {
        "id": "IYmjAmCXaWCS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "lWDJjBjiaGYg"
      },
      "outputs": [],
      "source": [
        "# 1. Carregar Dados do Kaggle\n",
        "def carregarDados(caminhoData: str) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Carrega e unifica os arquivos CSV do dataset de HRV via 'uuid'.\n",
        "    \"\"\"\n",
        "\n",
        "    dataframesBrutos = {}\n",
        "    try:\n",
        "        arquivosCsv = [f for f in os.listdir(caminhoData) if f.endswith('.csv')]\n",
        "        for arquivo in arquivosCsv:\n",
        "            nomeChave = arquivo.replace('.csv', '')\n",
        "            dataframesBrutos[nomeChave] = pd.read_csv(os.path.join(caminhoData, arquivo))\n",
        "\n",
        "        listaChaves = list(dataframesBrutos.keys())\n",
        "        dfUnificado = dataframesBrutos[listaChaves[0]]\n",
        "        for chave in listaChaves[1:]:\n",
        "            dfUnificado = pd.merge(dfUnificado, dataframesBrutos[chave], on='uuid', how='inner')\n",
        "\n",
        "        print(f\"✅ Dataset carregado! E unificado:\\n    {dfUnificado.shape}\")\n",
        "        return dfUnificado\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Erro no carregamento: {e}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "# 2. Seleção e Filtragem dos Dados\n",
        "def selecionarPreditoresETarget(dfEntrada: pd.DataFrame):\n",
        "    \"\"\"\n",
        "    Isola os preditores, remove classes intermediárias e equilibra as amostras\n",
        "    para evitar viés na classificação binária.\n",
        "    \"\"\"\n",
        "\n",
        "    # 1. Filtragem: Focar nos extremos (No Stress vs Time Pressure)\n",
        "    # Removemos 'interruption' para limpar a fronteira de decisão\n",
        "    dfLimpo = dfEntrada[dfEntrada['condition'] != 'interruption'].copy()\n",
        "\n",
        "    # 2. Mapeamento Binário\n",
        "    mapeamentoClasses = {'no stress': 0, 'time pressure': 1}\n",
        "    y = dfLimpo['condition'].map(mapeamentoClasses).values\n",
        "\n",
        "\n",
        "    # 3. Seleção de Features Estratégicas\n",
        "    featuresFinais = [\n",
        "                       'MEAN_RR', 'RMSSD', 'SDRR_REL_RR', 'RMSSD_REL_RR', # tempo\n",
        "                       'VLF', 'LF', 'HF', 'LF_HF',                        # frequencia\n",
        "                       'SD1', 'SD2', 'higuci'                             # nao linear\n",
        "                      ]\n",
        "\n",
        "    x = dfLimpo[featuresFinais].copy()\n",
        "\n",
        "    print(f\"➡️ Filtragem e mapeamento dos dados concluído.\")\n",
        "\n",
        "    return x, y\n",
        "\n",
        "\n",
        "# 3. Divisão dos Conjuntos de Dados\n",
        "def dividirDados(xDados, yAlvo, proporcaoTeste=0.2):\n",
        "    \"\"\"\n",
        "    Apenas separa os dados em conjuntos de treino e teste de forma estratificada.\n",
        "    \"\"\"\n",
        "\n",
        "    xTreino, xTeste, yTreino, yTeste = train_test_split(\n",
        "        xDados, yAlvo, test_size=proporcaoTeste, random_state=42, stratify=yAlvo\n",
        "    )\n",
        "    print(f\"➡️ Divisão dos conjuntos concluída (Teste: {proporcaoTeste*100}%).\")\n",
        "    return xTreino, xTeste, yTreino, yTeste\n",
        "\n",
        "\n",
        "# 4. Transformação e normalização dos dados\n",
        "def aplicarTransformacoes(xTreino, xTeste):\n",
        "    \"\"\"\n",
        "    Aplica transformações para aproximar a distribuição normal e padroniza as escalas.\n",
        "    Utiliza Yeo-Johnson para corrigir assimetria e StandardScaler para Z-score.\n",
        "    \"\"\"\n",
        "\n",
        "    # 1. Transformação de Potência (Para aproximar da normalidade multivariada)\n",
        "    # O método Yeo-Johnson é ideal para corrigir a assimetria (skewness) das features de HRV!!\n",
        "    powerTrans = PowerTransformer(method='yeo-johnson')\n",
        "\n",
        "    # 2. Padronização (Média 0, Variância 1)\n",
        "    scaler = StandardScaler()\n",
        "\n",
        "    # Aplicamos o pipeline de transformação\n",
        "    # Nota: fit() apenas no treino para evitar vazamento de informação!\n",
        "    xTreinoTrans = powerTrans.fit_transform(xTreino)\n",
        "    xTreinoFinal = scaler.fit_transform(xTreinoTrans)\n",
        "\n",
        "    xTesteTrans = powerTrans.transform(xTeste)\n",
        "    xTesteFinal = scaler.transform(xTesteTrans)\n",
        "\n",
        "    print(\"➡️ Transformação Yeo-Johnson e Padronização Z-score aplicadas.\")\n",
        "    return xTreinoFinal, xTesteFinal"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# >> Pipeline de Execução\n",
        "\n",
        "# 1. Download e definição de caminhos\n",
        "path = kagglehub.dataset_download(\"vinayakshanawad/heart-rate-prediction-to-monitor-stress-level\")\n",
        "caminhoDados = os.path.join(path, 'Train Data', 'Train Data Zip')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z68UYvRzkdUo",
        "outputId": "dcfe3e03-f0cc-4cf4-8fec-e3ff82a54b0b"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using Colab cache for faster access to the 'heart-rate-prediction-to-monitor-stress-level' dataset.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Ingestão e Unificação\n",
        "dfHrv = carregarDados(caminhoDados)\n",
        "\n",
        "if dfHrv is not None:\n",
        "    # 3. Seleção de Features e Mapeamento do Target (Binário)\n",
        "    # Aqui a função retorna os dados brutos ainda, apenas selecionados\n",
        "    xBruto, yAlvo = selecionarPreditoresETarget(dfHrv)\n",
        "\n",
        "    # 4. Divisão do Conjunto de Dados (Estratificada)\n",
        "    # Fundamental dividir ANTES de qualquer transformação para evitar Data Leakage!!!\n",
        "    xTreinoBruto, xTesteBruto, yTreino, yTeste = dividirDados(xBruto, yAlvo, proporcaoTeste=0.2)\n",
        "\n",
        "    # 5. Transformação (Normalidade via Yeo-Johnson + Padronização Z-score)\n",
        "    # Agora aplicamos a matemática para atender às premissas do LDA\n",
        "    xTreino, xTeste = aplicarTransformacoes(xTreinoBruto, xTesteBruto)\n",
        "\n",
        "    print(\"\\n✅ Pipeline concluído com sucesso!\")"
      ],
      "metadata": {
        "id": "zMuR3F3ygSOy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e5bcccd8-1454-4dd6-c6e0-644185536bc8"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Dataset carregado! E unificado:\n",
            "    (369289, 37)\n",
            "➡️ Filtragem e mapeamento dos dados concluído.\n",
            "➡️ Divisão dos conjuntos concluída (Teste: 20.0%).\n",
            "➡️ Transformação Yeo-Johnson e Padronização Z-score aplicadas.\n",
            "\n",
            "✅ Pipeline concluído com sucesso!\n"
          ]
        }
      ]
    }
  ]
}