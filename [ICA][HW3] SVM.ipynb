{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# [ICA][HW3] Support Vector Machine (SVM)\n",
        "\n",
        "Implementacao manual de SVM (margem suave) com gradiente descendente, seguindo o mesmo pipeline de pre-processamento dos outros notebooks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "40b5874b",
      "metadata": {},
      "outputs": [],
      "source": [
        "import kagglehub\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, PowerTransformer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "bcc10859",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 1. Carregar Dados do Kaggle\n",
        "def carregarDados(caminhoData: str) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Carrega e unifica os arquivos CSV do dataset de HRV via 'uuid'.\n",
        "    \"\"\"\n",
        "\n",
        "    dataframesBrutos = {}\n",
        "    try:\n",
        "        arquivosCsv = [f for f in os.listdir(caminhoData) if f.endswith('.csv')]\n",
        "        for arquivo in arquivosCsv:\n",
        "            nomeChave = arquivo.replace('.csv', '')\n",
        "            dataframesBrutos[nomeChave] = pd.read_csv(os.path.join(caminhoData, arquivo))\n",
        "\n",
        "        listaChaves = list(dataframesBrutos.keys())\n",
        "        dfUnificado = dataframesBrutos[listaChaves[0]]\n",
        "        for chave in listaChaves[1:]:\n",
        "            dfUnificado = pd.merge(dfUnificado, dataframesBrutos[chave], on='uuid', how='inner')\n",
        "\n",
        "        print(f\"Dataset carregado! E unificado:\\n    {dfUnificado.shape}\")\n",
        "        return dfUnificado\n",
        "    except Exception as e:\n",
        "        print(f\"Erro no carregamento: {e}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "\n",
        "# 2. Selecao e Filtragem dos Dados\n",
        "def selecionarPreditoresETarget(dfEntrada: pd.DataFrame):\n",
        "    \"\"\"\n",
        "    Isola os preditores, remove classes intermediarias e elimina variaveis\n",
        "    com multicolinearidade extrema (correlacao > 0.95).\n",
        "    \"\"\"\n",
        "\n",
        "    # 1. Filtragem: Focar nos extremos (No Stress vs Time Pressure)\n",
        "    # Removemos 'interruption' para limpar a fronteira de decisao e\n",
        "    # realizar uma classificacao binaria, apenas\n",
        "    dfLimpo = dfEntrada[dfEntrada['condition'] != 'interruption'].copy()\n",
        "    dfLimpo = dfLimpo.reset_index(drop=True)\n",
        "\n",
        "    # 2. Mapeamento Binario\n",
        "    mapeamentoClasses = {'no stress': 0, 'time pressure': 1}\n",
        "    y = dfLimpo['condition'].map(mapeamentoClasses).values\n",
        "\n",
        "    # 3. Identificacao Automatica de Preditores Numericos\n",
        "    # Removemos colunas nao-preditoras e o proprio alvo\n",
        "    colunasExcluir = ['uuid', 'datasetId', 'condition', 'target']\n",
        "    xBruto = dfLimpo.drop(columns=[c for c in colunasExcluir if c in dfLimpo.columns])\n",
        "    xBruto = xBruto.select_dtypes(include=[np.number])\n",
        "\n",
        "    # 4. Filtro de Multicolinearidade (Estrategia Inteligente)\n",
        "    # Calculamos a matriz de correlacao absoluta\n",
        "    matrizCorr = xBruto.corr().abs()\n",
        "\n",
        "    # Selecionamos o triangulo superior da matriz para identificar pares\n",
        "    superior = matrizCorr.where(np.triu(np.ones(matrizCorr.shape), k=1).astype(bool))\n",
        "\n",
        "    # Identificamos colunas com correlacao acima de 0.95\n",
        "    colunasParaRemover = [coluna for coluna in superior.columns if any(superior[coluna] > 0.95)]\n",
        "\n",
        "    x = xBruto.drop(columns=colunasParaRemover)\n",
        "\n",
        "    print(f\"Filtragem concluida.\\n  Preditores finais: {x.shape[1]} (Removidas {len(colunasParaRemover)} redundantes).\")\n",
        "\n",
        "    return x, y\n",
        "\n",
        "# 2.1 Equilibrio de Amostras (Opcional)\n",
        "def aplicarSubamostragem(xDados, yAlvo):\n",
        "    \"\"\"\n",
        "    Realiza o Undersampling aleatorio da classe majoritaria para equilibrar o dataset.\n",
        "    Retorna X e Y balanceados (proporcao 50/50).\n",
        "    \"\"\"\n",
        "\n",
        "    # Identificamos os indices de cada classe\n",
        "    indicesClasse0 = np.where(yAlvo == 0)[0]\n",
        "    indicesClasse1 = np.where(yAlvo == 1)[0]\n",
        "\n",
        "    # Determinamos o tamanho da menor classe (geralmente Estresse)\n",
        "    n_minoria = len(indicesClasse1)\n",
        "\n",
        "    # Sorteamos aleatoriamente a mesma quantidade na classe majoritaria\n",
        "    np.random.seed(27)\n",
        "    indicesClasse0_Reduzidos = np.random.choice(indicesClasse0, n_minoria, replace=False)\n",
        "\n",
        "    # Combinamos os indices e extraimos os dados\n",
        "    indicesFinais = np.concatenate([indicesClasse0_Reduzidos, indicesClasse1])\n",
        "    np.random.shuffle(indicesFinais)\n",
        "\n",
        "    xBalanceado = xDados.iloc[indicesFinais] if isinstance(xDados, pd.DataFrame) else xDados[indicesFinais]\n",
        "    yBalanceado = yAlvo[indicesFinais]\n",
        "\n",
        "    print(f\"Subamostragem concluida: {len(yBalanceado)} amostras totais (50/50).\")\n",
        "\n",
        "    return xBalanceado, yBalanceado\n",
        "\n",
        "\n",
        "# 3. Divisao dos Conjuntos de Dados\n",
        "def dividirDados(xDados, yAlvo, proporcaoTeste=0.2):\n",
        "    \"\"\"\n",
        "    Apenas separa os dados em conjuntos de treino e teste de forma estratificada.\n",
        "    \"\"\"\n",
        "\n",
        "    xTreino, xTeste, yTreino, yTeste = train_test_split(\n",
        "        xDados, yAlvo, test_size=proporcaoTeste, random_state=42, stratify=yAlvo\n",
        "    )\n",
        "    print(f\"Divisao dos conjuntos concluida (Teste: {proporcaoTeste*100}%).\")\n",
        "    return xTreino, xTeste, yTreino, yTeste\n",
        "\n",
        "\n",
        "# 4. Transformacao e normalizacao dos dados\n",
        "def aplicarTransformacoes(xTreino, xTeste):\n",
        "    \"\"\"\n",
        "    Aplica transformacoes para aproximar a distribuicao normal e padroniza as escalas.\n",
        "    Utiliza Yeo-Johnson para corrigir assimetria e StandardScaler para Z-score.\n",
        "    \"\"\"\n",
        "\n",
        "    # 1. Transformacao de Potencia (Para aproximar da normalidade multivariada)\n",
        "    powerTrans = PowerTransformer(method='yeo-johnson')\n",
        "\n",
        "    # 2. Padronizacao (Media 0, Variancia 1)\n",
        "    scaler = StandardScaler()\n",
        "\n",
        "    # Aplicamos o pipeline de transformacao\n",
        "    # Nota: fit() apenas no treino para evitar vazamento de informacao!\n",
        "    xTreinoTrans = powerTrans.fit_transform(xTreino)\n",
        "    xTreinoFinal = scaler.fit_transform(xTreinoTrans)\n",
        "\n",
        "    xTesteTrans = powerTrans.transform(xTeste)\n",
        "    xTesteFinal = scaler.transform(xTesteTrans)\n",
        "\n",
        "    print(\"Transformacao Yeo-Johnson e padronizacao z-score aplicadas.\")\n",
        "    return xTreinoFinal, xTesteFinal\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "d7bd3e4b",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.13), please consider upgrading to the latest version (0.4.0).\n",
            "Dataset carregado! E unificado:\n",
            "    (369289, 37)\n",
            "Filtragem concluida.\n",
            "  Preditores finais: 23 (Removidas 11 redundantes).\n",
            "Subamostragem concluida: 128114 amostras totais (50/50).\n",
            "Divisao dos conjuntos concluida (Teste: 20.0%).\n",
            "Transformacao Yeo-Johnson e padronizacao z-score aplicadas.\n",
            "\n",
            "Pipeline concluido com sucesso!\n"
          ]
        }
      ],
      "source": [
        "# >> Pipeline de Execucao\n",
        "\n",
        "# 1. Download e definicao de caminhos\n",
        "path = kagglehub.dataset_download(\"vinayakshanawad/heart-rate-prediction-to-monitor-stress-level\")\n",
        "caminhoDados = os.path.join(path, 'Train Data', 'Train Data Zip')\n",
        "\n",
        "# 2. Ingestao e Unificacao\n",
        "dfHrv = carregarDados(caminhoDados)\n",
        "\n",
        "if dfHrv is not None:\n",
        "    # 3. Selecao de Features e Mapeamento do Target (Binario)\n",
        "    xBruto, yAlvo = selecionarPreditoresETarget(dfHrv)\n",
        "\n",
        "    # 3.1 Aplicacao da Subamostragem\n",
        "    xBalanceado, yBalanceado = aplicarSubamostragem(xBruto, yAlvo)\n",
        "\n",
        "    # 4. Divisao do Conjunto de Dados (Estratificada)\n",
        "    xTreinoBruto, xTesteBruto, yTreino, yTeste = dividirDados(\n",
        "        xBalanceado, yBalanceado, proporcaoTeste=0.2\n",
        "    )\n",
        "\n",
        "    # 5. Transformacao (Normalidade via Yeo-Johnson + Padronizacao Z-score)\n",
        "    xTreino, xTeste = aplicarTransformacoes(xTreinoBruto, xTesteBruto)\n",
        "\n",
        "    print(\"\\nPipeline concluido com sucesso!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "9d335aa3",
      "metadata": {},
      "outputs": [],
      "source": [
        "class MapeadorRBF:\n",
        "    \"\"\"\n",
        "    Aproximacao do kernel RBF via Random Fourier Features.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, gamma=0.1, n_componentes=300, random_state=42):\n",
        "        self.gamma = gamma\n",
        "        self.n_componentes = n_componentes\n",
        "        self.random_state = random_state\n",
        "        self.W = None\n",
        "        self.b = None\n",
        "\n",
        "    def ajustar(self, xTreino):\n",
        "        n_features = xTreino.shape[1]\n",
        "        rng = np.random.default_rng(self.random_state)\n",
        "        self.W = rng.normal(scale=np.sqrt(2 * self.gamma), size=(n_features, self.n_componentes))\n",
        "        self.b = rng.uniform(0.0, 2 * np.pi, size=self.n_componentes)\n",
        "        return self\n",
        "\n",
        "    def transformar(self, xDados):\n",
        "        z = xDados @ self.W + self.b\n",
        "        return np.sqrt(2.0 / self.n_componentes) * np.cos(z)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "ddb2b1c5",
      "metadata": {},
      "outputs": [],
      "source": [
        "class MeuClassificadorSVM:\n",
        "    \"\"\"\n",
        "    Implementacao manual de SVM linear com margem suave (hinge loss).\n",
        "    Otimizado via gradiente descendente com regularizacao L2.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, taxaAprendizado=0.001, epochs=2000, C=1.0, batch_size=256,\n",
        "                 decay=0.001, patience=8, tol=1e-4, avaliar_intervalo=10):\n",
        "        self.taxaAprendizado = taxaAprendizado\n",
        "        self.epochs = epochs\n",
        "        self.C = C\n",
        "        self.batch_size = batch_size\n",
        "        self.decay = decay\n",
        "        self.patience = patience\n",
        "        self.tol = tol\n",
        "        self.avaliar_intervalo = avaliar_intervalo\n",
        "        self.w = None\n",
        "        self.b = 0.0\n",
        "\n",
        "    def _preparar_rotulos(self, y):\n",
        "        # Converte {0,1} -> {-1, +1}\n",
        "        y = np.array(y)\n",
        "        return np.where(y == 1, 1, -1)\n",
        "\n",
        "    def _calcular_loss(self, xDados, yDados):\n",
        "        margens = yDados * (np.dot(xDados, self.w) + self.b)\n",
        "        hinge = np.maximum(0.0, 1.0 - margens)\n",
        "        return 0.5 * np.dot(self.w, self.w) + self.C * np.mean(hinge)\n",
        "\n",
        "    def treinarModelo(self, xTreino, yTreino):\n",
        "        y = self._preparar_rotulos(yTreino)\n",
        "        nAmostras, nPreditores = xTreino.shape\n",
        "        self.w = np.zeros(nPreditores)\n",
        "        self.b = 0.0\n",
        "\n",
        "        best_w = None\n",
        "        best_b = 0.0\n",
        "        best_loss = float('inf')\n",
        "        sem_melhora = 0\n",
        "\n",
        "        # Treinamento por mini-batches\n",
        "        for epoch in range(self.epochs):\n",
        "            lr = self.taxaAprendizado / (1.0 + self.decay * epoch)\n",
        "            indices = np.random.permutation(nAmostras)\n",
        "            xShuffled = xTreino[indices]\n",
        "            yShuffled = y[indices]\n",
        "\n",
        "            for start in range(0, nAmostras, self.batch_size):\n",
        "                end = start + self.batch_size\n",
        "                xBatch = xShuffled[start:end]\n",
        "                yBatch = yShuffled[start:end]\n",
        "\n",
        "                margens = yBatch * (np.dot(xBatch, self.w) + self.b)\n",
        "                viol = margens < 1\n",
        "\n",
        "                # Gradiente do hinge loss + regularizacao L2\n",
        "                grad_w = self.w - self.C * np.mean(yBatch[:, None] * xBatch * viol[:, None], axis=0)\n",
        "                grad_b = -self.C * np.mean(yBatch * viol)\n",
        "\n",
        "                self.w -= lr * grad_w\n",
        "                self.b -= lr * grad_b\n",
        "\n",
        "            if (epoch + 1) % self.avaliar_intervalo == 0:\n",
        "                loss = self._calcular_loss(xTreino, y)\n",
        "                if best_loss - loss > self.tol:\n",
        "                    best_loss = loss\n",
        "                    best_w = self.w.copy()\n",
        "                    best_b = self.b\n",
        "                    sem_melhora = 0\n",
        "                else:\n",
        "                    sem_melhora += 1\n",
        "                    if sem_melhora >= self.patience:\n",
        "                        if best_w is not None:\n",
        "                            self.w = best_w\n",
        "                            self.b = best_b\n",
        "                        print(f\"Early stopping em epoch {epoch + 1}.\")\n",
        "                        break\n",
        "\n",
        "        print(\"Treino SVM concluido.\")\n",
        "\n",
        "    def predizer(self, xTeste):\n",
        "        scores = np.dot(xTeste, self.w) + self.b\n",
        "        return np.where(scores >= 0, 1, 0)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "4325b8f1",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Early stopping em epoch 90.\n",
            "Treino SVM concluido.\n",
            "gamma=0.03 n=300 C=1.0 acc_val=66.79%\n",
            "Early stopping em epoch 90.\n",
            "Treino SVM concluido.\n",
            "gamma=0.03 n=300 C=2.0 acc_val=66.76%\n",
            "Early stopping em epoch 100.\n",
            "Treino SVM concluido.\n",
            "gamma=0.03 n=300 C=5.0 acc_val=66.79%\n",
            "Early stopping em epoch 90.\n",
            "Treino SVM concluido.\n",
            "gamma=0.03 n=600 C=1.0 acc_val=67.06%\n",
            "Early stopping em epoch 90.\n",
            "Treino SVM concluido.\n",
            "gamma=0.03 n=600 C=2.0 acc_val=67.05%\n",
            "Early stopping em epoch 100.\n",
            "Treino SVM concluido.\n",
            "gamma=0.03 n=600 C=5.0 acc_val=67.04%\n",
            "Early stopping em epoch 90.\n",
            "Treino SVM concluido.\n",
            "gamma=0.05 n=300 C=1.0 acc_val=67.07%\n",
            "Early stopping em epoch 90.\n",
            "Treino SVM concluido.\n",
            "gamma=0.05 n=300 C=2.0 acc_val=67.13%\n",
            "Early stopping em epoch 100.\n",
            "Treino SVM concluido.\n",
            "gamma=0.05 n=300 C=5.0 acc_val=67.06%\n",
            "Early stopping em epoch 90.\n",
            "Treino SVM concluido.\n",
            "gamma=0.05 n=600 C=1.0 acc_val=67.73%\n",
            "Early stopping em epoch 90.\n",
            "Treino SVM concluido.\n",
            "gamma=0.05 n=600 C=2.0 acc_val=67.74%\n",
            "Early stopping em epoch 100.\n",
            "Treino SVM concluido.\n",
            "gamma=0.05 n=600 C=5.0 acc_val=67.79%\n",
            "Early stopping em epoch 90.\n",
            "Treino SVM concluido.\n",
            "gamma=0.1 n=300 C=1.0 acc_val=70.25%\n",
            "Early stopping em epoch 90.\n",
            "Treino SVM concluido.\n",
            "gamma=0.1 n=300 C=2.0 acc_val=69.98%\n",
            "Early stopping em epoch 100.\n",
            "Treino SVM concluido.\n",
            "gamma=0.1 n=300 C=5.0 acc_val=70.00%\n",
            "Early stopping em epoch 90.\n",
            "Treino SVM concluido.\n",
            "gamma=0.1 n=600 C=1.0 acc_val=72.32%\n",
            "Early stopping em epoch 90.\n",
            "Treino SVM concluido.\n",
            "gamma=0.1 n=600 C=2.0 acc_val=72.28%\n",
            "Early stopping em epoch 100.\n",
            "Treino SVM concluido.\n",
            "gamma=0.1 n=600 C=5.0 acc_val=72.24%\n",
            "Early stopping em epoch 90.\n",
            "Treino SVM concluido.\n",
            "gamma=0.2 n=300 C=1.0 acc_val=79.31%\n",
            "Early stopping em epoch 90.\n",
            "Treino SVM concluido.\n",
            "gamma=0.2 n=300 C=2.0 acc_val=79.30%\n",
            "Early stopping em epoch 100.\n",
            "Treino SVM concluido.\n",
            "gamma=0.2 n=300 C=5.0 acc_val=79.28%\n",
            "Early stopping em epoch 90.\n",
            "Treino SVM concluido.\n",
            "gamma=0.2 n=600 C=1.0 acc_val=83.09%\n",
            "Early stopping em epoch 90.\n",
            "Treino SVM concluido.\n",
            "gamma=0.2 n=600 C=2.0 acc_val=83.27%\n",
            "Early stopping em epoch 100.\n",
            "Treino SVM concluido.\n",
            "gamma=0.2 n=600 C=5.0 acc_val=83.32%\n",
            "Melhor configuracao: gamma=0.2 n=600 C=5.0 acc_val=83.32%\n",
            "Early stopping em epoch 90.\n",
            "Treino SVM concluido.\n"
          ]
        }
      ],
      "source": [
        "# 6. Busca de hiperparametros (gamma, n_componentes, C) com validacao\n",
        "np.random.seed(42)\n",
        "xTreinoBusca, xValBusca, yTreinoBusca, yValBusca = train_test_split(\n",
        "    xTreino, yTreino, test_size=0.2, random_state=42, stratify=yTreino\n",
        ")\n",
        "\n",
        "gammas = [0.03, 0.05, 0.1, 0.2]\n",
        "componentes = [300, 600]\n",
        "Cs = [1.0, 2.0, 5.0]\n",
        "epochs_busca = 800\n",
        "\n",
        "best = {'acc': -1.0, 'gamma': None, 'n_componentes': None, 'C': None}\n",
        "\n",
        "for gamma in gammas:\n",
        "    for n in componentes:\n",
        "        mapeador = MapeadorRBF(gamma=gamma, n_componentes=n, random_state=42)\n",
        "        mapeador.ajustar(xTreinoBusca)\n",
        "        xTreinoBuscaMap = mapeador.transformar(xTreinoBusca)\n",
        "        xValBuscaMap = mapeador.transformar(xValBusca)\n",
        "\n",
        "        for C in Cs:\n",
        "            modelo = MeuClassificadorSVM(\n",
        "                taxaAprendizado=0.001, epochs=epochs_busca, C=C, batch_size=256\n",
        "            )\n",
        "            modelo.treinarModelo(xTreinoBuscaMap, yTreinoBusca)\n",
        "            yValPred = modelo.predizer(xValBuscaMap)\n",
        "            acc = np.mean(yValPred == yValBusca)\n",
        "            print(f\"gamma={gamma} n={n} C={C} acc_val={acc:.2%}\")\n",
        "\n",
        "            if acc > best['acc']:\n",
        "                best = {'acc': acc, 'gamma': gamma, 'n_componentes': n, 'C': C}\n",
        "\n",
        "print(\n",
        "    f\"Melhor configuracao: gamma={best['gamma']} n={best['n_componentes']} C={best['C']} acc_val={best['acc']:.2%}\"\n",
        ")\n",
        "\n",
        "# Treino final com o melhor conjunto\n",
        "mapeador = MapeadorRBF(\n",
        "    gamma=best['gamma'], n_componentes=best['n_componentes'], random_state=42\n",
        ")\n",
        "mapeador.ajustar(xTreino)\n",
        "xTreinoMap = mapeador.transformar(xTreino)\n",
        "xTesteMap = mapeador.transformar(xTeste)\n",
        "\n",
        "modeloSvm = MeuClassificadorSVM(taxaAprendizado=0.001, epochs=2000, C=best['C'], batch_size=256)\n",
        "modeloSvm.treinarModelo(xTreinoMap, yTreino)\n",
        "\n",
        "yPreditoTreino = modeloSvm.predizer(xTreinoMap)\n",
        "yPreditoTeste = modeloSvm.predizer(xTesteMap)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "35d81cc8",
      "metadata": {},
      "outputs": [],
      "source": [
        "def calcularMetricas(yReal, yPrevisto):\n",
        "    \"\"\"\n",
        "    Calcula metricas de desempenho para classificacao binaria e exibe\n",
        "    um relatorio estruturado no console.\n",
        "    \"\"\"\n",
        "    # 1. Calculos de Base (Matriz de Confusao)\n",
        "    tp = np.sum((yReal == 1) & (yPrevisto == 1))\n",
        "    tn = np.sum((yReal == 0) & (yPrevisto == 0))\n",
        "    fp = np.sum((yReal == 0) & (yPrevisto == 1))\n",
        "    fn = np.sum((yReal == 1) & (yPrevisto == 0))\n",
        "\n",
        "    # 2. Calculo das Metricas Derivadas\n",
        "    acuracia = (tp + tn) / (tp + tn + fp + fn)\n",
        "\n",
        "    # Sensibilidade (Recall): Capacidade de detectar a classe 1 (Estresse)\n",
        "    sensibilidade = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
        "\n",
        "    # Especificidade: Capacidade de detectar a classe 0 (Calma)\n",
        "    especificidade = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
        "\n",
        "    # Precisao: Quando o modelo diz que e estresse, qual a chance de ser verdade?\n",
        "    precisao = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
        "\n",
        "    # F1-Score: Media harmonica entre precisao e sensibilidade\n",
        "    f1 = 2 * (precisao * sensibilidade) / (precisao + sensibilidade) if (precisao + sensibilidade) > 0 else 0\n",
        "\n",
        "    # 3. Prints Organizados e Estruturados\n",
        "    print(\"Relatorio de Desempenho da SVM\")\n",
        "    print(\"=\"*45)\n",
        "\n",
        "    print(f\"{'Metrica':<25} | {'Valor':<15}\")\n",
        "    print(\"-\" * 45)\n",
        "    print(f\"{'Acuracia Global':<25} | {acuracia:.2%}\")\n",
        "    print(f\"{'Sensibilidade (Recall)':<25} | {sensibilidade:.2%}\")\n",
        "    print(f\"{'Especificidade':<25} | {especificidade:.2%}\")\n",
        "    print(f\"{'Precisao':<25} | {precisao:.2%}\")\n",
        "    print(f\"{'F1-Score':<25} | {f1:.4f}\")\n",
        "\n",
        "    print(\"\\n\\n\")\n",
        "    print(\"Matriz de Confusao\")\n",
        "    print(\"-\" * 45)\n",
        "    print(f\"{'':>18} | {'Previsto: 0':<12} | {'Previsto: 1':<12}\")\n",
        "    print(f\"{'Real: 0 (Calma)':>18} | {tn:<12} | {fp:<12}\")\n",
        "    print(f\"{'Real: 1 (Estresse)':>18} | {fn:<12} | {tp:<12}\")\n",
        "    print(\"=\"*45 + \"\\n\")\n",
        "\n",
        "    matrizConfusao = np.array([[tn, fp], [fn, tp]])\n",
        "    return acuracia, matrizConfusao\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "6aff298a",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Treino\n",
            "Relatorio de Desempenho da SVM\n",
            "=============================================\n",
            "Metrica                   | Valor          \n",
            "---------------------------------------------\n",
            "Acuracia Global           | 83.45%\n",
            "Sensibilidade (Recall)    | 86.22%\n",
            "Especificidade            | 80.69%\n",
            "Precisao                  | 81.70%\n",
            "F1-Score                  | 0.8390\n",
            "\n",
            "\n",
            "\n",
            "Matriz de Confusao\n",
            "---------------------------------------------\n",
            "                   | Previsto: 0  | Previsto: 1 \n",
            "   Real: 0 (Calma) | 41349        | 9896        \n",
            "Real: 1 (Estresse) | 7062         | 44184       \n",
            "=============================================\n",
            "\n",
            "Teste\n",
            "Relatorio de Desempenho da SVM\n",
            "=============================================\n",
            "Metrica                   | Valor          \n",
            "---------------------------------------------\n",
            "Acuracia Global           | 83.51%\n",
            "Sensibilidade (Recall)    | 86.15%\n",
            "Especificidade            | 80.87%\n",
            "Precisao                  | 81.83%\n",
            "F1-Score                  | 0.8393\n",
            "\n",
            "\n",
            "\n",
            "Matriz de Confusao\n",
            "---------------------------------------------\n",
            "                   | Previsto: 0  | Previsto: 1 \n",
            "   Real: 0 (Calma) | 10361        | 2451        \n",
            "Real: 1 (Estresse) | 1774         | 11037       \n",
            "=============================================\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# 4. Calcular metricas finais no treino\n",
        "print('Treino')\n",
        "acuraciaGeralTreino, matrizConfusaoTreino = calcularMetricas(yTreino, yPreditoTreino)\n",
        "\n",
        "# 4. Calcular metricas finais no teste\n",
        "print('Teste')\n",
        "acuraciaGeralTeste, matrizConfusaoTeste = calcularMetricas(yTeste, yPreditoTeste)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
