# Documenta√ß√£o: Rede Neural para Predi√ß√£o de Heart Rate

## üìã Sum√°rio
1. [Arquitetura da Rede Neural](#arquitetura)
2. [Implementa√ß√£o T√©cnica](#implementa√ß√£o)
3. [Processo de Treinamento](#treinamento)
4. [Avalia√ß√£o do Modelo](#avalia√ß√£o)
5. [Exporta√ß√£o e Carregamento](#persist√™ncia)

---

## üèóÔ∏è Arquitetura {#arquitetura}

### Estrutura Geral

A rede neural implementada √© um **modelo de 2 camadas** (duas-camadas feed-forward):

```
Entrada (X: n√óD)
    ‚Üì
Camada Oculta (128 neur√¥nios, Sigmoide)
    ‚Üì
Camada de Sa√≠da (1 neur√¥nio, Linear)
    ‚Üì
Predi√ß√£o (≈∑: n√ó1)
```

**Onde:**
- **D** = n√∫mero de features (vari√°veis preditoras) = 19 features ap√≥s pr√©-processamento
- **n** = n√∫mero de exemplos de treinamento
- **Camada Oculta**: 128 neur√¥nios com ativa√ß√£o **Sigmoide** para capturar n√£o-linearidades
- **Camada de Sa√≠da**: 1 neur√¥nio com ativa√ß√£o **Linear** (apropriado para regress√£o)

### Dimens√µes dos Pesos

```
W1 (entrada ‚Üí oculta):  (19, 128)
b1 (bias oculta):       (1, 128)

W2 (oculta ‚Üí sa√≠da):    (128, 1)
b2 (bias sa√≠da):        (1, 1)

Total de par√¢metros: 19√ó128 + 128 + 128√ó1 + 1 = 2,561 par√¢metros
```

---

## üíª Implementa√ß√£o T√©cnica {#implementa√ß√£o}

### Classe: `NeuralNetworkRegression`

#### 1. **Inicializa√ß√£o** (`__init__`)

```python
def __init__(self, input_size, hidden_size, output_size=1, weight_decay=0.0):
```

**Responsabilidades:**
- Inicializa os pesos W1 e W2 com distribui√ß√£o uniforme $\mathcal{U}[-0.7, 0.7]$
- Inicializa os biases b1 e b2 com zeros
- Armazena o par√¢metro de regulariza√ß√£o L2 (`weight_decay`)

**Decis√µes de Design:**
- Intervalo de inicializa√ß√£o [-0.7, 0.7] √© recomendado para dados padronizados
- Biases come√ßam em zero (inicializa√ß√£o padr√£o)
- Weight decay permite controlar a magnitude dos pesos durante o treinamento

#### 2. **Forward Pass** (`forward`)

```python
def forward(self, X):
```

**Fluxo Computacional:**

1. **Camada Oculta:**
   $$z_1 = X \cdot W_1 + b_1$$
   $$a_1 = \sigma(z_1) = \frac{1}{1 + e^{-z_1}}$$

2. **Camada de Sa√≠da:**
   $$z_2 = a_1 \cdot W_2 + b_2$$
   $$\hat{y} = z_2 \quad \text{(sem ativa√ß√£o para regress√£o)}$$

**Detalhes Importantes:**
- For√ßa convers√£o de X para `float64` para evitar erros num√©ricos (overflow em sigmoid)
- Armazena internamente `z1`, `a1`, `z2` para uso no backward pass
- N√£o aplica sigmoide na sa√≠da (regress√£o linear, n√£o classifica√ß√£o)

#### 3. **Backward Pass** (`backward`)

Implementa o algoritmo de **backpropagation** com c√°lculo de gradientes:

**Etapa 1: Erro da Sa√≠da**
$$\delta_{out} = \hat{y} - y = \text{(Predi√ß√£o - Real)}$$

**Etapa 2: Retropropaga√ß√£o do Erro**
$$\delta_{hidden} = (\delta_{out} \cdot W_2^T) \odot \sigma'(a_1)$$

Onde $\sigma'(a_1) = a_1 \cdot (1 - a_1)$ √© a derivada da sigmoide.

**Etapa 3: C√°lculo dos Gradientes**
$$\frac{\partial L}{\partial W_2} = \frac{1}{m} a_1^T \cdot \delta_{out}$$
$$\frac{\partial L}{\partial b_2} = \frac{1}{m} \sum \delta_{out}$$
$$\frac{\partial L}{\partial W_1} = \frac{1}{m} X^T \cdot \delta_{hidden}$$
$$\frac{\partial L}{\partial b_1} = \frac{1}{m} \sum \delta_{hidden}$$

**Etapa 4: Atualiza√ß√£o dos Pesos (Gradient Descent com L2 Regulariza√ß√£o)**
$$W_2 := W_2 - \alpha \left(\frac{\partial L}{\partial W_2} + \lambda W_2\right)$$
$$W_1 := W_1 - \alpha \left(\frac{\partial L}{\partial W_1} + \lambda W_1\right)$$

Onde:
- $\alpha$ = taxa de aprendizado (learning_rate)
- $\lambda$ = weight_decay (par√¢metro de regulariza√ß√£o L2)

#### 4. **Fun√ß√£o de Ativa√ß√£o Sigmoide**

```python
def _sigmoid(self, z):
    z = np.asarray(z, dtype=np.float64)
    return 1 / (1 + np.exp(-z))
```

**Propriedades:**
- Range: (0, 1)
- Fun√ß√£o n√£o-linear que introduz capacidade de modelar rela√ß√µes complexas
- For√ßa convers√£o para float64 para estabilidade num√©rica (evita overflow)

#### 5. **Derivada da Sigmoide**

```python
def _sigmoid_derivative(self, a):
    return a * (1 - a)
```

Usa a propriedade: $\frac{d}{dz}\sigma(z) = \sigma(z) \cdot (1 - \sigma(z))$

---

## üöÄ Processo de Treinamento {#treinamento}

### M√©todo: `train()`

```python
def train(self, X, y, epochs, learning_rate):
```

**Pseudoc√≥digo:**
```
Para cada √©poca (1 at√© epochs):
    1. Forward pass: calcular ≈∑
    2. Calcular loss (MSE)
    3. Backward pass: calcular gradientes
    4. Atualizar pesos via gradient descent
```

**Par√¢metros Utilizados:**

| Par√¢metro | Valor | Justificativa |
|-----------|-------|---------------|
| Hidden Size | 128 | Melhor desempenho encontrado via Grid Search de hiperpar√¢metros |
| Learning Rate | 0.01 | Velocidade de converg√™ncia moderada |
| Weight Decay | 0.001 | Regulariza√ß√£o L2 leve para reduzir overfitting |
| Epochs | 2000 | Suficiente para converg√™ncia |

**Loss Function (MSE):**
$$L = \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2$$

**Monitoramento:**
- Imprime loss a cada 1000 √©pocas
- Retorna hist√≥rico completo de loss para visualiza√ß√£o

---

## üìä Avalia√ß√£o do Modelo {#avalia√ß√£o}

### M√©tricas Utilizadas

#### 1. **R¬≤ Score (Coeficiente de Determina√ß√£o)**

$$R^2 = 1 - \frac{\sum(y_i - \hat{y}_i)^2}{\sum(y_i - \bar{y})^2}$$

**Interpreta√ß√£o:**
- $R^2 = 1$: Predi√ß√µes perfeitas
- $R^2 = 0$: Modelo n√£o melhor que predizer a m√©dia
- $R^2 < 0$: Modelo pior que a baseline

**Classifica√ß√£o de Performance:**
| Faixa | Qualidade |
|-------|-----------|
| R¬≤ > 0.9 | Excelente |
| 0.8 < R¬≤ ‚â§ 0.9 | Muito Bom |
| 0.7 < R¬≤ ‚â§ 0.8 | Bom |
| 0.5 < R¬≤ ‚â§ 0.7 | Aceit√°vel |
| R¬≤ ‚â§ 0.5 | Fraco |

#### 2. **RMSE (Root Mean Squared Error)**

$$\text{RMSE} = \sqrt{\frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2}$$

**Unidade:** bpm (batidas por minuto)

**Interpreta√ß√£o:** Erro m√©dio em valores reais da vari√°vel alvo.

### Avalia√ß√£o em Notebook

No arquivo `treinar_rede_neural.ipynb`, a avalia√ß√£o segue este fluxo:

```python
# 1. Fazer predi√ß√µes
y_train_pred = model.forward(X_train)
y_test_pred = model.forward(X_test)

# 2. Calcular R¬≤ e RMSE
train_r2 = r2_score(YTrain, y_train_pred)
test_r2 = r2_score(YTest, y_test_pred)

train_rmse = np.sqrt(mean_squared_error(YTrain, y_train_pred))
test_rmse = np.sqrt(mean_squared_error(YTest, y_test_pred))

# 3. Detec√ß√£o de Overfitting
overfitting = abs(train_r2 - test_r2)
```

### Visualiza√ß√µes

A avalia√ß√£o inclui 4 gr√°ficos:

1. **Scatter: Predito vs Real (Treino)**
   - Pontos pr√≥ximos √† diagonal = boas predi√ß√µes
   - Dispers√£o = incerteza do modelo

2. **Scatter: Predito vs Real (Teste)**
   - Valida generaliza√ß√£o do modelo
   - Comparar com treino detecta overfitting

3. **Distribui√ß√£o de Res√≠duos (Treino)**
   - Res√≠duo = valor real - predito
   - Distribui√ß√£o centrada em 0 √© desej√°vel
   - Mostra if modelo tem vi√©s sistem√°tico

4. **Distribui√ß√£o de Res√≠duos (Teste)**
   - Confirma que res√≠duos em teste tamb√©m s√£o normais
   - Assimetrias indicam problemas no modelo

---

## üîç Valida√ß√£o de Hiperpar√¢metros {#validacao}

A etapa de valida√ß√£o testa m√∫ltiplas configura√ß√µes via **Grid Search**:

### Grid de Busca

```python
hidden_sizes = [32, 64, 128]
learning_rates = [0.001, 0.01]
weight_decays = [0.0, 0.001]
epochs_val = 300
```

**Total:** 3 √ó 2 √ó 2 = 12 combina√ß√µes

### Estrat√©gia de Valida√ß√£o

1. **Divis√£o de Dados:** Treino (80%) + Valida√ß√£o (20%)
2. **Para cada combina√ß√£o:**
   - Treinar modelo com valida√ß√£o split
   - Avaliar R¬≤ e RMSE no conjunto de valida√ß√£o
3. **Sele√ß√£o:** Configura√ß√£o com maior R¬≤ na valida√ß√£o
4. **Retorno:** Melhores par√¢metros para treinamento final

---

## üíæ Exporta√ß√£o e Carregamento {#persist√™ncia}

### Salvar Modelo

```python
model.save_model('modelos_treinados/modelo_hr_prediction.npz')
```

**Formato:** NPZ (NumPy compressed archive)

**Conte√∫do Preservado:**
```
W1:           Pesos camada 1 (19, 64)
b1:           Biases camada 1 (1, 64)
W2:           Pesos camada 2 (64, 1)
b2:           Biases camada 2 (1, 1)
weight_decay: Hiperpar√¢metro de regulariza√ß√£o
```

### Carregar Modelo

```python
modelo_carregado = NeuralNetworkRegression.load_model(
    'modelos_treinados/modelo_hr_prediction.npz'
)
```

**Processo:**
1. L√™ arquivo NPZ
2. Extrai dimens√µes de W1 e W2 ‚Üí reconstr√≥i arquitetura (19 ‚Üí 128 ‚Üí 1)
3. Carrega todos os pesos treinados
4. Retorna inst√¢ncia pronta para predi√ß√£o

**Verifica√ß√£o de Integridade:**
```python
# Compara predi√ß√µes antes/depois
diff = np.abs(y_pred_original - y_pred_carregado).max()
# Deve ser pr√≥ximo de 0
```

---

## üìà Exemplo de Uso Completo

### Em novo notebook:

```python
from neural_network import NeuralNetworkRegression
import numpy as np

# 1. Carregar dados processados
X_novo = pd.read_csv('dados_processados/XTest.csv').values

# 2. Carregar modelo treinado
modelo = NeuralNetworkRegression.load_model(
    'modelos_treinados/modelo_hr_prediction.npz'
)

# 3. Fazer predi√ß√µes
predicoes = modelo.forward(X_novo)

# 4. Obter informa√ß√µes do modelo
info = modelo.get_model_info()
print(f"Arquitetura: {info['input_size']} ‚Üí {info['hidden_size']} ‚Üí {info['output_size']}")
print(f"Total de par√¢metros: {info['total_params']}")
# Sa√≠da esperada: 19 ‚Üí 128 ‚Üí 1 (2561 par√¢metros)
```

---

## üîß Decis√µes de Design e Justificativas

| Decis√£o | Justificativa |
|---------|---------------|
| 2 camadas (n√£o mais) | Suficiente para dados tabulares; evita overfitting |
| Sigmoide na oculta | N√£o-linearidade; evita colapso para linear simples |
| Sem sigmoide na sa√≠da | Regress√£o: precisamos valores cont√≠nuos reais |
| Inicializa√ß√£o [-0.7, 0.7] | Padr√£o para dados padronizados; evita satura√ß√£o inicial |
| Weight decay (L2) | Regulariza√ß√£o: penaliza pesos grandes, reduz overfitting |
| Learning rate 0.01 | Balance: r√°pido o suficiente, est√°vel o bastante |
| MSE como loss | Apropriado para regress√£o; diferenci√°vel |

---

## ‚ö†Ô∏è Considera√ß√µes e Limita√ß√µes

1. **Dados Padronizados:** X deve ser padronizado (m√©dia=0, std=1) antes de treinar
2. **Escala de Y:** N√£o padronizamos Y; modelo aprende a escala direto
3. **Sem Dropout:** N√£o usamos dropout; confiamos em weight decay
4. **Batch Size:** Usamos batch completo (todas as amostras por √©poca)
5. **Learning Rate Fixo:** Sem decay; poderia melhorar com scheduler
6. **Sem Valida√ß√£o Cross-Fold:** Usamos single train/test split

---

## üéØ Pr√≥ximos Passos Recomendados

1. **Tuning Avan√ßado:** Testar epochs maiores (5000+) e learning rates menores
2. **Early Stopping:** Parar treinamento se val_loss n√£o melhorar
3. **Batch Normalization:** Estabilizar training e acelerar converg√™ncia
4. **Dropout:** Reduzir overfitting ainda mais
5. **Arquiteturas Alternativas:** Testar 3+ camadas
6. **Ensemble:** Combinar m√∫ltiplos modelos

---

**√öltima atualiza√ß√£o:** 8 de dezembro de 2025
**Arquivo relacionado:** `neural_network.py`, `treinar_rede_neural.ipynb`
