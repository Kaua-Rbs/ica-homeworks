{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# PARTE 0 | Importes Necessários"
      ],
      "metadata": {
        "id": "V_LLfWXdaM9z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import kagglehub\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, PowerTransformer"
      ],
      "metadata": {
        "id": "3nwsAdtkaVZ0"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PARTE 1 | Conjunto de Dados"
      ],
      "metadata": {
        "id": "IYmjAmCXaWCS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "lWDJjBjiaGYg"
      },
      "outputs": [],
      "source": [
        "# 1. Carregar Dados do Kaggle\n",
        "def carregarDados(caminhoData: str) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Carrega e unifica os arquivos CSV do dataset de HRV via 'uuid'.\n",
        "    \"\"\"\n",
        "\n",
        "    dataframesBrutos = {}\n",
        "    try:\n",
        "        arquivosCsv = [f for f in os.listdir(caminhoData) if f.endswith('.csv')]\n",
        "        for arquivo in arquivosCsv:\n",
        "            nomeChave = arquivo.replace('.csv', '')\n",
        "            dataframesBrutos[nomeChave] = pd.read_csv(os.path.join(caminhoData, arquivo))\n",
        "\n",
        "        listaChaves = list(dataframesBrutos.keys())\n",
        "        dfUnificado = dataframesBrutos[listaChaves[0]]\n",
        "        for chave in listaChaves[1:]:\n",
        "            dfUnificado = pd.merge(dfUnificado, dataframesBrutos[chave], on='uuid', how='inner')\n",
        "\n",
        "        print(f\"Dataset carregado! E unificado:\\n    {dfUnificado.shape}\")\n",
        "        return dfUnificado\n",
        "    except Exception as e:\n",
        "        print(f\"Erro no carregamento: {e}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "\n",
        "# 2. Seleção e Filtragem dos Dados\n",
        "def selecionarPreditoresETarget(dfEntrada: pd.DataFrame):\n",
        "    \"\"\"\n",
        "    Isola os preditores, remove classes intermediárias e elimina variáveis\n",
        "    com multicolinearidade extrema (correlação > 0.95).\n",
        "    \"\"\"\n",
        "\n",
        "    # 1. Filtragem: Focar nos extremos (No Stress vs Time Pressure)\n",
        "    # Removemos 'interruption' para limpar a fronteira de decisão e\n",
        "    # realizar uma classificação binária, apenas\n",
        "    dfLimpo = dfEntrada[dfEntrada['condition'] != 'interruption'].copy()\n",
        "    dfLimpo = dfLimpo.reset_index(drop=True)\n",
        "\n",
        "    # 2. Mapeamento Binário\n",
        "    mapeamentoClasses = {'no stress': 0, 'time pressure': 1}\n",
        "    y = dfLimpo['condition'].map(mapeamentoClasses).values\n",
        "\n",
        "    # 3. Identificação Automática de Preditores Numéricos\n",
        "    # Removemos colunas não-preditoras e o próprio alvo\n",
        "    colunasExcluir = ['uuid', 'datasetId', 'condition', 'target']\n",
        "    xBruto = dfLimpo.drop(columns=[c for c in colunasExcluir if c in dfLimpo.columns])\n",
        "    xBruto = xBruto.select_dtypes(include=[np.number]) # Garante apenas números\n",
        "\n",
        "    # 4. Filtro de Multicolinearidade (Estratégia Inteligente)\n",
        "    # Calculamos a matriz de correlação absoluta\n",
        "    matrizCorr = xBruto.corr().abs()\n",
        "\n",
        "    # Selecionamos o triângulo superior da matriz para identificar pares\n",
        "    superior = matrizCorr.where(np.triu(np.ones(matrizCorr.shape), k=1).astype(bool))\n",
        "\n",
        "    # Identificamos colunas com correlação acima de 0.95\n",
        "    colunasParaRemover = [coluna for coluna in superior.columns if any(superior[coluna] > 0.95)]\n",
        "\n",
        "    x = xBruto.drop(columns=colunasParaRemover)\n",
        "\n",
        "    print(f\"Filtragem concluída.\\n  Preditores finais: {x.shape[1]} (Removidas {len(colunasParaRemover)} redundantes).\")\n",
        "\n",
        "    return x, y\n",
        "\n",
        "# 2.1 Equilíbrio de Amostras (Opcional)\n",
        "def aplicarSubamostragem(xDados, yAlvo):\n",
        "    \"\"\"\n",
        "    Realiza o Undersampling aleatório da classe majoritária para equilibrar o dataset.\n",
        "    Retorna X e Y balanceados (proporção 50/50).\n",
        "    \"\"\"\n",
        "\n",
        "    # Identificamos os índices de cada classe\n",
        "    indicesClasse0 = np.where(yAlvo == 0)[0]\n",
        "    indicesClasse1 = np.where(yAlvo == 1)[0]\n",
        "\n",
        "    # Determinamos o tamanho da menor classe (geralmente Estresse)\n",
        "    n_minoria = len(indicesClasse1)\n",
        "\n",
        "    # Sorteamos aleatoriamente a mesma quantidade na classe majoritária\n",
        "    np.random.seed(27) # Para reprodutibilidade\n",
        "    indicesClasse0_Reduzidos = np.random.choice(indicesClasse0, n_minoria, replace=False)\n",
        "\n",
        "    # Combinamos os índices e extraímos os dados\n",
        "    indicesFinais = np.concatenate([indicesClasse0_Reduzidos, indicesClasse1])\n",
        "    np.random.shuffle(indicesFinais) # Mistura os dados\n",
        "\n",
        "    xBalanceado = xDados.iloc[indicesFinais] if isinstance(xDados, pd.DataFrame) else xDados[indicesFinais]\n",
        "    yBalanceado = yAlvo[indicesFinais]\n",
        "\n",
        "    print(f\"Subamostragem concluída: {len(yBalanceado)} amostras totais (50/50).\")\n",
        "\n",
        "    return xBalanceado, yBalanceado\n",
        "\n",
        "\n",
        "# 3. Divisão dos Conjuntos de Dados\n",
        "def dividirDados(xDados, yAlvo, proporcaoTeste=0.2):\n",
        "    \"\"\"\n",
        "    Apenas separa os dados em conjuntos de treino e teste de forma estratificada.\n",
        "    \"\"\"\n",
        "\n",
        "    xTreino, xTeste, yTreino, yTeste = train_test_split(\n",
        "        xDados, yAlvo, test_size=proporcaoTeste, random_state=42, stratify=yAlvo\n",
        "    )\n",
        "    print(f\"Divisão dos conjuntos concluída (Teste: {proporcaoTeste*100}%).\")\n",
        "    return xTreino, xTeste, yTreino, yTeste\n",
        "\n",
        "\n",
        "# 4. Transformação e normalização dos dados\n",
        "def aplicarTransformacoes(xTreino, xTeste):\n",
        "    \"\"\"\n",
        "    Aplica transformações para aproximar a distribuição normal e padroniza as escalas.\n",
        "    Utiliza Yeo-Johnson para corrigir assimetria e StandardScaler para Z-score.\n",
        "    \"\"\"\n",
        "\n",
        "    # 1. Transformação de Potência (Para aproximar da normalidade multivariada)\n",
        "    # O método Yeo-Johnson é ideal para corrigir a assimetria (skewness) das features de HRV!!\n",
        "    powerTrans = PowerTransformer(method='yeo-johnson')\n",
        "\n",
        "    # 2. Padronização (Média 0, Variância 1)\n",
        "    scaler = StandardScaler()\n",
        "\n",
        "    # Aplicamos o pipeline de transformação\n",
        "    # Nota: fit() apenas no treino para evitar vazamento de informação!\n",
        "    xTreinoTrans = powerTrans.fit_transform(xTreino)\n",
        "    xTreinoFinal = scaler.fit_transform(xTreinoTrans)\n",
        "\n",
        "    xTesteTrans = powerTrans.transform(xTeste)\n",
        "    xTesteFinal = scaler.transform(xTesteTrans)\n",
        "\n",
        "    print(\"Transformação Yeo-Johnson e padronização z-score aplicadas.\")\n",
        "    return xTreinoFinal, xTesteFinal"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# >> Pipeline de Execução\n",
        "\n",
        "# 1. Download e definição de caminhos\n",
        "path = kagglehub.dataset_download(\"vinayakshanawad/heart-rate-prediction-to-monitor-stress-level\")\n",
        "caminhoDados = os.path.join(path, 'Train Data', 'Train Data Zip')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z68UYvRzkdUo",
        "outputId": "075e2222-2f19-4492-c0fa-b0b508fc610a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.13), please consider upgrading to the latest version (0.4.0).\n",
            "Downloading from https://www.kaggle.com/api/v1/datasets/download/vinayakshanawad/heart-rate-prediction-to-monitor-stress-level?dataset_version_number=1...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 140M/140M [00:00<00:00, 154MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting files...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Ingestão e Unificação\n",
        "dfHrv = carregarDados(caminhoDados)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ip4qLcyflZEz",
        "outputId": "1ec1cf86-031f-4fd1-ac06-831b8a95f1e8"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset carregado! E unificado:\n",
            "    (369289, 37)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if dfHrv is not None:\n",
        "    # 3. Seleção de Features e Mapeamento do Target (Binário)\n",
        "    # Aqui a função retorna os dados brutos ainda, apenas selecionados\n",
        "    xBruto, yAlvo = selecionarPreditoresETarget(dfHrv)\n",
        "\n",
        "    # 3.1 Aplicação da Subamostragem\n",
        "    xBalanceado, yBalanceado = aplicarSubamostragem(xBruto, yAlvo)\n",
        "\n",
        "    # 4. Divisão do Conjunto de Dados (Estratificada)\n",
        "    # Fundamental dividir ANTES de qualquer transformação para evitar Data Leakage!!!\n",
        "    xTreinoBruto, xTesteBruto, yTreino, yTeste = dividirDados(xBalanceado, yBalanceado, proporcaoTeste=0.2)\n",
        "\n",
        "    # 5. Transformação (Normalidade via Yeo-Johnson + Padronização Z-score)\n",
        "    # Agora aplicamos a matemática para atender às premissas do LDA\n",
        "    xTreino, xTeste = aplicarTransformacoes(xTreinoBruto, xTesteBruto)\n",
        "\n",
        "    print(\"\\nPipeline concluído com sucesso!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zMuR3F3ygSOy",
        "outputId": "21a34277-73ce-4f42-9ff2-31180d3146be"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Filtragem concluída.\n",
            "  Preditores finais: 23 (Removidas 11 redundantes).\n",
            "Subamostragem concluída: 128114 amostras totais (50/50).\n",
            "Divisão dos conjuntos concluída (Teste: 20.0%).\n",
            "Transformação Yeo-Johnson e padronização z-score aplicadas.\n",
            "\n",
            "Pipeline concluído com sucesso!\n"
          ]
        }
      ]
    }
  ]
}